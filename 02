2.1. Predicting Price with Size

import matplotlib.pyplot as plt
import pandas as pd
import wqet_grader
from IPython.display import VimeoVideo
from sklearn.linear_model import LinearRegression #build the model
from sklearn.metrics import mean_absolute_error #evaluate the moodel
from sklearn.utils.validation import check_is_fitted


2.1.1 建立函数function
def wrangle(filepath):
    #read CSV file into dataframe
    df = pd.read_csv(filepath)
    return df

df = wrangle("data/buenos-aires-real-estate-1.csv")
print("df shape:", df.shape)
df.head()

# Check your work
assert (
    len(df) <= 8606
), f"`df` should have no more than 8606 observations, not {len(df)}."

#unique
df[x].unique()
返回该列有多少种不同的值，列出不同的值

2.1.3
#建立筛选条件的函数
def wrangle(filepath):
    #read CSV file into dataframe
    df = pd.read_csv(filepath)
    
    #subset to properties in "Capital Federal"
    mask_ba = df["place_with_parent_names"].str.contains("Capital Federal")
    
    #subset to "apartments"
    mask_apt = df["property_type"] == "apartment"
    
    #subset to "properties"
    mask_price = df["price_aprox_usd"] < 400000
    
    df = df[mask_ba & mask_apt & mask_price]
    
    return df

2.1.4 Create a histogram
plt.hist(df["surface_covered_in_m2"])
plt.xlabel("Area [sq meters]")
plt.title("Distribution of Apartment Sizes");


2.1.5 
对整个表的各列进行描述性统计
df.describe()

对某一列进行描述性统计
df.describe()["surface_covered_in_m2"]

2.1.6删除“surface_covered_in_m2”列中异常值的观察结果。具体来说，所有观察值都应介于“surface_covered_in_m2”的 0.1 和 0.9 分位数之间
def wrangle(filepath):
    #read CSV file into dataframe
    df = pd.read_csv(filepath)
    
    #subset to properties in "Capital Federal"
    mask_ba = df["place_with_parent_names"].str.contains("Capital Federal")
    
    #subset to "apartments"
    mask_apt = df["property_type"] == "apartment"
    
    #subset to properties where "price_aprox_usd"< 400000
    mask_price = df["price_aprox_usd"] < 400000
    
    df = df[mask_ba & mask_apt & mask_price]
    
    #remove outliers by "surface_couvered_in_m2"
    low,high = df["surface_covered_in_m2"].quantile([0.1,0.9])
    mask_area = df["surface_covered_in_m2"].between(low,high)
    
    df = df[mask_area]
    
    return df

         #split
#linear regression线性回归

2.1.8 创建名为 X_train 的特征矩阵
只包含["surface_covered_in_m2"]一个特征
x--->features 特征
Y--->vector向量

features = ["surface_covered_in_m2"]
X_train = df[features]

2.1.9 Create the target vector named y_train
target = "price_aprox_usd"
y_train = df[target]

2.1.10 baselining 基线

2.1.11 创建一个名为 y_pred_baseline 的列表，其中包含重复的 y_mean 值，使其在 y 处的长度相同


2.1.12 把基线画到散点图上


2.1.13 平均绝对误差 (MAE)
绝对误差是单次测量的误差，平均绝对误差是多次测量过程中的平均误差。
MAE 越接近 0，我们的模型就越适合数据。

mae_baseline = mean_absolute_error(y_train,y_pred_baseline)

print("Mean apt price", round(y_mean, 2))
print("Baseline MAE:", round(mae_baseline, 2))

Iterate
2.1.14
迭代涉及构建模型、训练模型、评估模型，然后重复该过程，直到您对模型的性能感到满意为止。

Linear regression 线性回归

实例化一个名为 model 的 LinearRegression 模型。
model = LinearRegression()

# Check your work
assert isinstance(model, LinearRegression)

2.1.15 Fit your model to the data, X_train and y_train.
model.fit(X_train, y_train)

# Check your work
check_is_fitted(model)

   evaluate
2.1.16  create a list of predictions for the observations in your feature matrix X_train.

y_pred_training = model.predict(X_train)
y_pred_training[:5]

# Check your work
assert (
    len(y_pred_training) == 1343
), f"There should be 1343 predictions in `y_pred_training`, not {len(y_pred_training)}."

2.1.17 Calculate your training mean absolute error for your predictions
mae_training = mean_absolute_error(y_train, y_pred_training)
print("Training MAE:", round(mae_training, 2))

2.1.18


Communicate Results

2.1.19 Extract the intercept from your model, and assign it to the variable intercept.

intercept = round(model.intercept_,2)
print("Model Intercept:", intercept)
assert any([isinstance(intercept, int), isinstance(intercept, float)])

2.1.20 Extract the coefficient
coefficient = round(model.coef_[0],2)
print('Model coefficient for "surface_covered_in_m2":', coefficient)
assert any([isinstance(coefficient, int), isinstance(coefficient, float)])

2.1.21 print out the complete equation
print(f"apt_price = {intercept} + {coefficient} * surface_couvered")

2.1.22 Add a line to the plot

plt.plot(X_train.values, model.predict(X_train), color="r", label="Linear Model")
plt.scatter(X_train, y_train)
plt.xlabel("surface covered [sq meters]")
plt.ylabel("price [usd]")
plt.legend();

2.2



2.3

2.3.1
# glob
glob.glob(pathname, *, root_dir=None, dir_fd=None, recursive=False, include_hidden=False)

# Use glob to create a list that contains the filenames for all the Buenos Aires real estate CSV files
files = glob("data/buenos-aires-real-estate-*.csv")
files

2.3.2 Use your wrangle function in a for loop to create a list named frames. 
frames = []
for file in files:
    df = wrangle(file)
    frames.append(df)

2.3.3 Use pd.concat to concatenate the items in frames into a single DataFrame df.
df = pd.concat(frames, ignore_index=True)
df.shape

2.3.4 Modify your wrangle function to create a new feature "neighborhood".

# extract neighborhood
    df["neighborhood"] = df["place_with_parent_names"].str.split("|",expand=True)[3].head()
    df.drop(columns="place_with_parent_names", inplace=True)


2.3.5 Create your feature matrix X_train and target vector y_train. 

target = "price_aprox_usd"
features = ["neighborhood"]
y_train = df[target]
X_train = df[features]

2.3.6 Calculate the baseline mean absolute error for your model.
y_mean = y_train.mean()
y_pred_baseline = [y_mean] * len(y_train)
print("Mean apt price:", y_mean)
print("Baseline MAE:", mean_absolute_error(y_train ,y_pred_baseline))

2.3.7 OneHotEncoder


#First, instantiate a OneHotEncoder named ohe. Make sure to set the use_cat_names argument to True.
ohe = OneHotEncoder(use_cat_names = True)

# Next, fit your transformer to the feature matrix X_train.
ohe.fit(X_train)

# Finally, use your encoder to transform the feature matrix X_train, and assign the transformed data to the variable XT_train.
XT_train = ohe.transform(X_train)
print(XT_train.shape)
XT_train.head()

2.3.8 Create a pipeline named model that contains a OneHotEncoder transformer and a LinearRegression predictor
model = make_pipeline(
    OneHotEncoder(use_cat_names = True),
    LinearRegression()
)

model.fit(X_train, y_train)

# Evaluate

2.3.9 First, create a list of predictions for the observations in your feature matrix X_train. Name this list y_pred_training. 
Then calculate the training mean absolute error for your predictions in y_pred_training as compared to the true targets in y_train.

y_pred_training = model.predict(X_train)
mae_training = mean_absolute_error(y_train, y_pred_training)
print("Training MAE:", round(mae_training, 2))

2.3.10 Run the code below to import your test data buenos-aires-test-features.csv into a DataFrame and generate a Series of predictions using your model.
X_test = pd.read_csv("data/buenos-aires-test-features.csv")[features]
y_pred_test = pd.Series(model.predict(X_test))
y_pred_test.head()

# Communicate Results

2.3.11 Extract the intercept and coefficients for your model.
intercept = model.named_steps["linearregression"].intercept_
coefficients = model.named_steps["linearregression"].coef_
print("coefficients len:", len(coefficients))
print(coefficients[:5])  # First five coefficients

2.3.12 Extract the feature names of your encoded data from the OneHotEncoder in your model.
feature_names = model.named_steps["onehotencoder"].get_feature_names()
print("features len:", len(feature_names))
print(feature_names[:5])  # First five feature names

2.3.13 Create a pandas Series named feat_imp where the index is your features and the values are your coefficients.
feat_imp = pd.Series(coefficients, index=feature_names)
feat_imp.head()

2.3.14 Run the cell below to print the equation that your model has determined for predicting apartment price based on longitude and latitude.
print(f"price = {intercept.round(2)}")
for f, c in feat_imp.items():
    print(f"+ ({round(c, 2)} * {f})")

2.3.15 change the predictor in your model to Ridge, and retrain it.

intercept = model.named_steps["ridge"].intercept_
coefficients = model.named_steps["ridge"].coef_

print("coefficients len:", len(coefficients))
print(coefficients[:5])  # First five coefficients

feat_imp = pd.Series(coefficients, index=feature_names)
feat_imp.head()

print(f"price = {intercept.round(2)}")
for f, c in feat_imp.items():
    print(f"+ ({round(c, 2)} * {f})")

2.3.16 Create a horizontal bar chart that shows the top 15 coefficients for your model, based on their absolute value.
feat_imp.sort_values(key=abs).tail(15).plot(kind="barh")
plt.xlabel("Importance [USD]")
plt.ylabel("Feature")
plt.title("Feature Importance for Apartment Price");


2.4

2.4.1: Use glob to create a list that contains the filenames for all the Buenos Aires real estate CSV files in the data directory. 
files = glob("data/buenos-aires-real-estate-*.csv")
files

 2.4.2: Use your wrangle function in a list comprehension to create a list named frames.
 
 # List comprehension 列表理解用于在不显式编写循环的情况下遍历列表，这对于根据特定条件过滤数据特别有用。
 
frames = [wrangle(file) for file in files]
frames[0].head()

2.4.3 Use pd.concat to concatenate it items in frames into a single DataFrame df. 
df = pd.concat(frames, ignore_index=True)
print(df.info())
df.head()

2.4.4 Modify your wrangle function to drop any columns that are more than half NaN values.
# 每个columns中的空值有多少个
df.isnull().sum()

def wrangle(filepath):
    # Read CSV file
    df = pd.read_csv(filepath)

    # Subset data: Apartments in "Capital Federal", less than 400,000
    mask_ba = df["place_with_parent_names"].str.contains("Capital Federal")
    mask_apt = df["property_type"] == "apartment"
    mask_price = df["price_aprox_usd"] < 400_000
    df = df[mask_ba & mask_apt & mask_price]

    # Subset data: Remove outliers for "surface_covered_in_m2"
    low, high = df["surface_covered_in_m2"].quantile([0.1, 0.9])
    mask_area = df["surface_covered_in_m2"].between(low, high)
    df = df[mask_area]

    # Split "lat-lon" column
    df[["lat", "lon"]] = df["lat-lon"].str.split(",", expand=True).astype(float)
    df.drop(columns="lat-lon", inplace=True)

    # Get place name
    df["neighborhood"] = df["place_with_parent_names"].str.split("|", expand=True)[3]
    df.drop(columns="place_with_parent_names", inplace=True)
    
    # drop features with high null counts
    df.drop(columns=["floor","expenses"],inplace=True)
    
    return df

2.4.5 Calculate the number of unique values for each non-numeric feature in df.

# low or high cardinality 低基数或高基数

# 看"object"的columns有哪些
df.select_dtypes("object").head()

#计算每列中不同的值有多少个
df.select_dtypes("object").nunique()

2.4.6: Modify your wrangle function to drop high- and low-cardinality categorical features.
# 对wrangle进行更改
def wrangle(filepath):
    # Read CSV file
    df = pd.read_csv(filepath)

    # Subset data: Apartments in "Capital Federal", less than 400,000
    mask_ba = df["place_with_parent_names"].str.contains("Capital Federal")
    mask_apt = df["property_type"] == "apartment"
    mask_price = df["price_aprox_usd"] < 400_000
    df = df[mask_ba & mask_apt & mask_price]

    # Subset data: Remove outliers for "surface_covered_in_m2"
    low, high = df["surface_covered_in_m2"].quantile([0.1, 0.9])
    mask_area = df["surface_covered_in_m2"].between(low, high)
    df = df[mask_area]

    # Split "lat-lon" column
    df[["lat", "lon"]] = df["lat-lon"].str.split(",", expand=True).astype(float)
    df.drop(columns="lat-lon", inplace=True)

    # Get place name
    df["neighborhood"] = df["place_with_parent_names"].str.split("|", expand=True)[3]
    df.drop(columns="place_with_parent_names", inplace=True)
    
    # drop features with high null counts
    df.drop(columns=["floor","expenses"],inplace=True)
    
    # drop low and high cardinality categorial variables
    df.drop(columns=["operation","property_type","currency","properati_url"],inplace=True)

    return df
    
2.4.7 Modify your wrangle function to drop any features that would constitute leakage.
# Leakage 渗漏量

# 列出每列的名字
sorted(df.columns)

def wrangle(filepath):
    # Read CSV file
    df = pd.read_csv(filepath)

    # Subset data: Apartments in "Capital Federal", less than 400,000
    mask_ba = df["place_with_parent_names"].str.contains("Capital Federal")
    mask_apt = df["property_type"] == "apartment"
    mask_price = df["price_aprox_usd"] < 400_000
    df = df[mask_ba & mask_apt & mask_price]

    # Subset data: Remove outliers for "surface_covered_in_m2"
    low, high = df["surface_covered_in_m2"].quantile([0.1, 0.9])
    mask_area = df["surface_covered_in_m2"].between(low, high)
    df = df[mask_area]

    # Split "lat-lon" column
    df[["lat", "lon"]] = df["lat-lon"].str.split(",", expand=True).astype(float)
    df.drop(columns="lat-lon", inplace=True)

    # Get place name
    df["neighborhood"] = df["place_with_parent_names"].str.split("|", expand=True)[3]
    df.drop(columns="place_with_parent_names", inplace=True)
    
    # drop features with high null counts
    df.drop(columns=["floor","expenses"],inplace=True)
    
    # drop low and high cardinality categorial variables
    df.drop(columns=["operation","property_type","currency","properati_url"],inplace=True)
    
    # drop leaky columns
    df.drop(columns=[
        'price',
        'price_aprox_local_currency',
        
        'price_per_m2',
        'price_usd_per_m2',],
        inplace=True
    )

    return df


# 多重共线性，使用heatmap检验特征矩阵中特征向量的相关程度。

2.4.8 Plot a correlation heatmap of the remaining numerical features in df. 
Since "price_aprox_usd" will be your target, you don't need to include it in your heatmap.

corr = df.select_dtypes("number").drop(columns = "price_aprox_usd").corr()
sns.heatmap(corr)

2.4.9: Modify your wrangle function to remove columns so that there are no strongly correlated features in your feature matrix.
    # drop columns with multicollinearlity
    df.drop(columns = ["surface_total_in_m2","rooms"],inplace=True)


2.4.10: Create your feature matrix X_train and target vector y_train. Your target is "price_aprox_usd".
target = "price_aprox_usd"
features = ["neighborhood","lat","lon","surface_covered_in_m2"]
y_train = df[target]
X_train = df[features]
X_train.shape


2.4.11 Calculate the baseline mean absolute error for your model.
y_mean = y_train.mean()
y_pred_baseline = [y_mean]*len(y_train)
print("Mean apt price:", round(y_mean, 2))
print("Baseline MAE:", mean_absolute_error(y_train, y_pred_baseline))

2.4.12 Create a pipeline named model that contains a OneHotEncoder, SimpleImputer, and Ridge predictor.

model = make_pipeline(
    OneHotEncoder(),
    SimpleImputer(),
    Ridge()
)

model.fit(X_train, y_train)

2.4.13: Calculate the training mean absolute error for your predictions as compared to the true targets in y_train.
y_pred_training = model.predict(X_train)

print("Training MAE:", mean_absolute_error(y_train, y_pred_training))

2.4.14 test
X_test = pd.read_csv("data/buenos-aires-test-features.csv")
y_pred_test = pd.Series(model.predict(X_test))
y_pred_test.head()

wqet_grader.grade("Project 2 Assessment", "Task 2.4.14", y_pred_test)

2.4.15: Create a function make_prediction that takes four arguments (area, lat, lon, and neighborhood) and returns your model's prediction for an apartment price.
def make_prediction(area, lat, lon, neighborhood):
    data = {
        "surface_covered_in_m2":area,
        "lat":lat,
        "lon":lon,
        "neighborhood":neighborhood
    }
    df = pd.DataFrame(data, index=[0])
    prediction = model.predict(df)
    
    return f"Predicted apartment price: ${prediction}"

2.4.16: Add your make_prediction to the interact widget below, run the cell, and then adjust the widget to see how predicted apartment price changes.
Create an interact function in Jupyter Widgets.

interact(
    make_prediction,
    area=IntSlider(
        min=X_train["surface_covered_in_m2"].min(),
        max=X_train["surface_covered_in_m2"].max(),
        value=X_train["surface_covered_in_m2"].mean(),
    ),
    lat=FloatSlider(
        min=X_train["lat"].min(),
        max=X_train["lat"].max(),
        step=0.01,
        value=X_train["lat"].mean(),
    ),
    lon=FloatSlider(
        min=X_train["lon"].min(),
        max=X_train["lon"].max(),
        step=0.01,
        value=X_train["lon"].mean(),
    ),
    neighborhood=Dropdown(options=sorted(X_train["neighborhood"].unique())),
);



2.3 assignment
## Prepare Data

1 # Import 

# Build your `wrangle` function
def wrangle(filepath):
    
    df = pd.read_csv(filepath)
    
    # Subset data 条件筛选
    make_am = df["place_with_parent_names"].str.contains("Distrito Federal")
    mask_apt = df["property_type"] == "apartment"
    mask_price = df["price_aprox_usd"] < 100_000
    df = df[make_am & mask_apt & mask_price]
    
    # Remove outliers by trimming the bottom and top 10% of properties 去头去尾
    low, high = df["surface_covered_in_m2"].quantile([0.1, 0.9])
    mask_area = df["surface_covered_in_m2"].between(low, high)
    df = df[mask_area]
    
    # Create separate "lat" and "lon" columns.  处理数据，分成两列
    df[["lat", "lon"]] = df["lat-lon"].str.split(",", expand=True).astype(float)
    df.drop(columns="lat-lon", inplace=True)
    
    # Create a "borough" feature from the "place_with_parent_names" column. 提取杂糅的一列中需要的内容
    df["borough"] = df["place_with_parent_names"].str.split("|", expand=True)[1]
    df.drop(columns="place_with_parent_names", inplace=True)
    
    # Drop columns that are more than 50% null values. 删除空值多的列
    df.drop(columns = ["floor","expenses","surface_total_in_m2","price_usd_per_m2","rooms"],inplace=True)
    
    #检索每列空值多少个    df.isnull().sum()
    
    # Drop columns containing low- or high-cardinality categorical values. 删除低基数或高基数的列，指一列中只有1钟值或者每列都必然不同无数据跟踪意义
    df.drop(columns =["operation","properati_url","currency","property_type"], inplace=True)
    
    
    # Drop any columns that would constitute leakage for the target "price_aprox_usd".
    df.drop(columns=[
        'price',
        'price_aprox_local_currency',
        
        'price_per_m2'],
        inplace=True
    )
    
    # Drop any columns that would create issues of multicollinearity. 删除具有多重共线性的列
   
    return df

# 批量打开csv文件，处理为dataframe数据

files = glob("data/mexico-city-real-estate-*.csv")

frames = [wrangle(file) for file in files] #用wrangle函数处理数据
frames[0].head()

df = pd.concat(frames, ignore_index=True) #将几个dataframe连接成一个

print(df.info())
df.head()

2、
# Explore

创建直方图
# Build histogram
plt.hist(df["price_aprox_usd"])

# Label axes
plt.xlabel("Price [$]")
plt.ylabel("Count")
# Add title
plt.title("Distribution of Apartment Prices");

创建散点图
# Build scatter plot
features = ["surface_covered_in_m2"]
X_train = df[features]
target = "price_aprox_usd"
y_train = df[target]

plt.scatter(X_train,y_train)

# Label axes
plt.xlabel("Area [sq meters]")
plt.ylabel("Price [USD]")

# Add title
plt.title("Mexico City: Price vs. Area");

创建地图价格指数
# Plot Mapbox location and price
fig = px.scatter_mapbox(
    df,
    lat="lat",
    lon="lon",
    width=600,  # Width of map
    height=600,  # Height of map
    color="price_aprox_usd",
    hover_data=["price_aprox_usd"],  # Display price when hovering mouse over house
)

fig.update_layout(mapbox_style="open-street-map")

fig.show()

3、Split

创建特征矩阵和目标向量
# Split data into feature matrix `X_train` and target vector `y_train`.
target = "price_aprox_usd" #一个序列
features = ["surface_covered_in_m2", "lat", "lon", "borough"] #一个矩阵列表

X_train = df[features]
y_train = df[target]

## Build Model

4、Baseline
计算模型的基线平均绝对误差。
y_mean = y_train.mean()
y_pred_baseline = [y_mean]*len(y_train)
baseline_mae = mean_absolute_error(y_train, y_pred_baseline)
print("Mean apt price:", y_mean)
print("Baseline MAE:", baseline_mae)

5、Iterate
建立一个包含转换数据、预测数据的传递途径的模型
# Build Model
model = make_pipeline(
    OneHotEncoder(use_cat_names = True),  # use_cat_names=True 使用该列的值作为新的features
    SimpleImputer(),
    Ridge()   
)
# Fit model
model.fit(X_train, y_train)

6、Evaluate 估计  # 确保用于训练模型的 X_train 与 X_test 具有相同的列顺序。否则，它可能会损害模型的性能。
X_test = pd.read_csv("data/mexico-city-test-features.csv")
print(X_test.info())
X_test.head()

y_test_pred = pd.Series(model.predict(X_test))
y_test_pred.head()

## Communicate Results

建立回归模型
intercept = model.named_steps["ridge"].intercept_    #截距
coefficients = model.named_steps["ridge"].coef_     #系数
features = model.named_steps["onehotencoder"].get_feature_names()
feat_imp = pd.Series(coefficients, index=features)
feat_imp

画图
# Build bar chart
feat_imp.sort_values(key=abs).tail(10).plot(kind="barh")
# Label axes
plt.xlabel("Importance [USD]")
plt.ylabel("Feature")
# Add title
plt.title("Feature Importances for Apartment Price")










